{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting without GPU support\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run this file to process an audio file with a particular model.\n",
    "\n",
    "Returns a csv file with prediction information and also a video file containing an animated prediction.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from scipy.signal import resample\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import json\n",
    "import os, sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('/Users/hajarat/Developer/Workspaces/Github/raw-audio-gender-classification'))\n",
    "\n",
    "from config import PATH, LIBRISPEECH_SAMPLING_RATE\n",
    "from models import *\n",
    "from utils import whiten\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "print('Predicting {} GPU support'.format('with' if torch.cuda.is_available() else 'without'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Parameters #\n",
    "##############\n",
    "\n",
    "model_path = PATH + '/models/max_pooling__n_layers=7__n_filters=64__downsampling=1__n_seconds=3.torch'\n",
    "audio_path = PATH + '/data/whitening_test_audio.flac'\n",
    "step_seconds = 0.04\n",
    "batchsize_for_prediction = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error opening '/Users/hajarat/Developer/Workspaces/Github/raw-audio-gender-classification/data/whitening_test_audio.flac': File contains data in an unimplemented format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b418f7cf9355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load audio #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m##############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_sampling_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0maudio_duration_seconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maudio_sampling_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maudio_duration_minutes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_duration_seconds\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/soundfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(file, frames, start, stop, dtype, always_2d, fill_value, out, samplerate, channels, format, subtype, endian, closefd)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \"\"\"\n\u001b[1;32m    256\u001b[0m     with SoundFile(file, 'r', samplerate, channels,\n\u001b[0;32m--> 257\u001b[0;31m                    subtype, endian, format, closefd) as f:\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    625\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    626\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid file: {0!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         _error_check(_snd.sf_error(file_ptr),\n\u001b[0;32m-> 1182\u001b[0;31m                      \"Error opening {0!r}: \".format(self.name))\n\u001b[0m\u001b[1;32m   1183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;31m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_error_check\u001b[0;34m(err, prefix)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error opening '/Users/hajarat/Developer/Workspaces/Github/raw-audio-gender-classification/data/whitening_test_audio.flac': File contains data in an unimplemented format."
     ]
    }
   ],
   "source": [
    "##############\n",
    "# Load audio #\n",
    "##############\n",
    "audio, audio_sampling_rate = sf.read(audio_path)\n",
    "audio_duration_seconds = audio.shape[0]*1./audio_sampling_rate\n",
    "audio_duration_minutes = audio_duration_seconds/60.\n",
    "print('Audio duration: {}s'.format(audio_duration_seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Load model #\n",
    "##############\n",
    "model_type = model_path.split('/')[-1].split('__')[0]\n",
    "model_name = model_path.split('/')[-1].split('.')[0]\n",
    "model_params = {i.split('=')[0]: float(i.split('=')[1]) for i in model_name.split('__')[1:]}\n",
    "\n",
    "# Here we assume that the model was trained on the LibriSpeech dataset\n",
    "model_sampling_rate = LIBRISPEECH_SAMPLING_RATE/model_params['downsampling']\n",
    "model_num_samples = int(model_params['n_seconds']*model_sampling_rate)\n",
    "\n",
    "print('Model parameters determined from filename:')\n",
    "print(json.dumps(model_params, indent=4))\n",
    "\n",
    "if model_type == 'max_pooling':\n",
    "    model = ConvNet(int(model_params['n_filters']), int(model_params['n_layers']))\n",
    "elif model_type == 'dilated':\n",
    "    model = DilatedNet(int(model_params['n_filters']), int(model_params['n_depth']), int(model_params['n_stacks']))\n",
    "else:\n",
    "    raise(ValueError, 'Model type not recognised.')\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.double()\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Loop through audio #\n",
    "######################\n",
    "step_samples = int(step_seconds*model_sampling_rate)\n",
    "step_samples_at_audio_rate = int(step_seconds*audio_sampling_rate)\n",
    "print('Making predictions every {}s'.format(step_seconds))\n",
    "print('This is every {} samples at the models sampling rate'.format(step_samples))\n",
    "print('This is every {} samples at the input audio\\'s sampling rate'.format(step_samples_at_audio_rate))\n",
    "\n",
    "print('Looping through audio...')\n",
    "default_shape = None\n",
    "batch = []\n",
    "pred = []\n",
    "for lower in tqdm(range(0, audio.shape[0]-(int(model_params['n_seconds']*audio_sampling_rate)), step_samples_at_audio_rate)):\n",
    "    x = audio[lower:lower+(int(model_params['n_seconds']*audio_sampling_rate))]\n",
    "\n",
    "    # Don't predict on the last bit of audio where the duration isn't large enough\n",
    "    if x.shape[0] != model_params['n_seconds']*audio_sampling_rate:\n",
    "        break\n",
    "\n",
    "    x = torch.from_numpy(x).reshape(1, -1)\n",
    "\n",
    "    x = whiten(x)\n",
    "\n",
    "    # For me the bottleneck is this scipy resample call, increasing batch size doesn't make it any faster\n",
    "    x = torch.from_numpy(\n",
    "        resample(x, model_num_samples, axis=1)\n",
    "    ).reshape((1, 1, model_num_samples))\n",
    "\n",
    "    y_hat = model(x).item()\n",
    "\n",
    "    pred.append(y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Create output dataframe #\n",
    "###########################\n",
    "segment_start_times_minutes = np.array(range(len(pred)))*step_seconds/60\n",
    "df = pd.DataFrame(data={'minute': segment_start_times_minutes, 'p': pred})\n",
    "df = df.assign(\n",
    "    second=df['minute'].apply(lambda m: (m % 1)*60),\n",
    "    # Time in seconds of the start of the prediction fragment\n",
    "    t_start=df['minute']*60,\n",
    "    # Time in seconds of the end of the prediction fragment\n",
    "    t_end=df['minute']*60 + model_params['n_seconds'],\n",
    "    # Time in seconds of the center of the prediction fragment\n",
    "    t_center=df['minute']*60 + model_params['n_seconds']/2.\n",
    ")\n",
    "df.to_csv(PATH+'/data/results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
